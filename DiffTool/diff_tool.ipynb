{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import builtins\n",
    "import json\n",
    "from types import FunctionType\n",
    "\n",
    "from PaladinEngine.archive.archive import Archive\n",
    "from PaladinUI.paladin_server.paladin_server import PaladinServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = \"diff_tool_input.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_archive_from_csv(csv_path):\n",
    "    dataframe = load_csv_to_dataframe(csv_path)\n",
    "    archive_from_csv = Archive()\n",
    "    rows = dataframe.to_records()\n",
    "    items_for_future_build = []\n",
    "    for row in rows: #TODO: look for a builtin function instead of loop?\n",
    "        record_key, record_value, is_build_needed = create_record_key_value(row)\n",
    "        if is_build_needed:\n",
    "            items_for_future_build.append((record_key, record_value))\n",
    "        else:\n",
    "            archive_from_csv.store_with_original_time(record_key, record_value)\n",
    "    # Build items\n",
    "    for record_key, record_value in items_for_future_build:\n",
    "        new_record_value = build_object_from_archive(archive_from_csv, record_value)\n",
    "        archive_from_csv.store_with_original_time(record_key, new_record_value)\n",
    "    return archive_from_csv\n",
    "\n",
    "def load_csv_to_dataframe(csv_path):\n",
    "    dataframe = pd.read_csv(csv_path)\n",
    "    dataframe = dataframe.fillna('')\n",
    "    return dataframe\n",
    "\n",
    "def create_record_key_value(row):\n",
    "    record_key = Archive.Record.RecordKey(int(row.container_id), row.field, row.stub_name, row.kind)\n",
    "    is_build_needed = False\n",
    "\n",
    "    if record_key.stub_name == \"__AS__\" and row.rtype in ['list', 'dict', 'tuple', 'set']:\n",
    "        value_of_record = ast.literal_eval(row.value)\n",
    "        type_of_record = row.rtype\n",
    "        is_build_needed = True\n",
    "    elif row.rtype == 'function':\n",
    "        value_of_record = row.value\n",
    "        type_of_record = FunctionType\n",
    "    elif row.rtype == 'list':\n",
    "        value_of_record = ast.literal_eval(row.value)\n",
    "        type_of_record = list\n",
    "    elif row.rtype == 'bool':\n",
    "        value_of_record = True if row.value == 'True' else False\n",
    "        type_of_record = bool\n",
    "    elif row.rtype == 'dict':\n",
    "        value_of_record = ast.literal_eval(row.value)\n",
    "        type_of_record = dict #TODO: add same for tuple and set\n",
    "    else:\n",
    "        value_of_record = getattr(builtins, row.rtype)(row.value)\n",
    "        type_of_record = type(value_of_record)\n",
    "\n",
    "    record_value = Archive.Record.RecordValue(\n",
    "        record_key, type_of_record, value_of_record, row.expression,\n",
    "        int(row.line_no), int(row.time), row.extra\n",
    "    )\n",
    "    return record_key, record_value, is_build_needed\n",
    "\n",
    "def build_object_from_archive(archive_from_csv, record_value):\n",
    "    print(f\"v.rtype: {record_value.rtype} | v.value: {record_value.value}\")\n",
    "    value_of_record = archive_from_csv.build_object(record_value.value, record_value.time)\n",
    "    record_value.value = value_of_record\n",
    "    record_value.rtype = type(value_of_record)\n",
    "    print(f\"record_value.rtype: {record_value.rtype} | value_of_record: {value_of_record}\")\n",
    "    return record_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_query_result_to_presentable_table(query_result):\n",
    "    result = query_result['result']['query']\n",
    "    print(result)\n",
    "    json_data = json.loads(result)\n",
    "    json_data.pop('keys')\n",
    "    result_df = pd.DataFrame.from_dict(json_data, orient=\"index\")\n",
    "    print(result_df)\n",
    "    result_df.reset_index(inplace=True)\n",
    "    times_column_name = 'Time Range'\n",
    "    result_df = result_df.rename(columns={'index': times_column_name})\n",
    "    print(result_df)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def get_parameter_matches_with_previous_query(parameters, csv_path):\n",
    "#     parameter_matches = {}\n",
    "#     for parameter in parameters:\n",
    "#         parameter_matches[parameter] = input(f\"Match {parameter} in {csv_path}: \")\n",
    "#     return parameter_matches\n",
    "#\n",
    "# def replace_matched_parameters(previous_query, parameter_matches):\n",
    "#     matched_query = previous_query\n",
    "#     for source_parameter, dest_parameter in parameter_matches.items():\n",
    "#         matched_query = matched_query.replace(source_parameter, dest_parameter)\n",
    "#     return matched_query\n",
    "#\n",
    "# def convert_parameter_to_queryable(parameter):\n",
    "#     location_separator = '@'\n",
    "#     if location_separator in parameter:\n",
    "#         parameter_name, parameter_location = parameter.split(location_separator)\n",
    "#         queryable = f\"[[{parameter_name}]]@{parameter_location}\"\n",
    "#     else:\n",
    "#         queryable = f\"[[{parameter}]]\"\n",
    "#     return queryable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataframes_from_csvs(csv_files):\n",
    "    archives = []\n",
    "    dataframes = []\n",
    "\n",
    "    for csv_file_info in csv_files:\n",
    "        csv_archive = create_archive_from_csv(csv_file_info[\"csv_file_path\"])\n",
    "        archives.append(csv_archive)\n",
    "        server = PaladinServer.create('', csv_archive)\n",
    "        raw_result = server.query(csv_file_info[\"query\"], csv_file_info[\"start_time\"], csv_file_info[\"end_time\"])\n",
    "        presentable_df = convert_query_result_to_presentable_table(raw_result)\n",
    "        print(presentable_df)\n",
    "        dataframes.append(presentable_df)\n",
    "\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(INPUT_FILE_PATH, 'r') as fileobj:\n",
    "        data = json.load(fileobj)\n",
    "    csv_files = data[\"csv_files\"]\n",
    "    dataframes = create_dataframes_from_csvs(csv_files)\n",
    "\n",
    "    result_merge_condition = data[\"result_merge_condition\"]\n",
    "    result_rows = pd.merge(\n",
    "        dataframes[0], dataframes[1],\n",
    "        how=\"inner\",\n",
    "        left_on=result_merge_condition[\"left_on\"], right_on=result_merge_condition[\"right_on\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    result_rows = result_rows[~((result_rows['result@12'].isna()) & (result_rows['result@16'].isna()))]\n",
    "    print(result_rows)\n",
    "\n",
    "    iteration_merge_condition = data[\"iteration_merge_condition\"]\n",
    "    iteration_rows = pd.merge(\n",
    "        dataframes[0], dataframes[1],\n",
    "        how=\"outer\",\n",
    "        left_on=iteration_merge_condition[\"left_on\"], right_on=iteration_merge_condition[\"right_on\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    iteration_rows = iteration_rows[((iteration_rows['result@12'].isna()) & (iteration_rows['result@16'].isna()))]\n",
    "    print(iteration_rows)\n",
    "\n",
    "    merged = pd.concat([iteration_rows, result_rows], ignore_index=True)\n",
    "    print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"(0, 1)\": {\"number@11\": [null], \"result@12\": [null], \"i@4\": [null]}, \"(3, 6)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": [null]}, \"(8, 8)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": 3}, \"(10, 10)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": 5}, \"(12, 12)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": 7}, \"(14, 14)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": 9}, \"(16, 16)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": 11}, \"(18, 18)\": {\"number@11\": 13, \"result@12\": [null], \"i@4\": 12}, \"(20, 500)\": {\"number@11\": 13, \"result@12\": true, \"i@4\": 12}, \"keys\": [\"number@11\", \"result@12\", \"i@4\"]}\n",
      "<class 'str'>\n",
      "{'(0, 1)': {'number@11': [None], 'result@12': [None], 'i@4': [None]}, '(3, 6)': {'number@11': 13, 'result@12': [None], 'i@4': [None]}, '(8, 8)': {'number@11': 13, 'result@12': [None], 'i@4': 3}, '(10, 10)': {'number@11': 13, 'result@12': [None], 'i@4': 5}, '(12, 12)': {'number@11': 13, 'result@12': [None], 'i@4': 7}, '(14, 14)': {'number@11': 13, 'result@12': [None], 'i@4': 9}, '(16, 16)': {'number@11': 13, 'result@12': [None], 'i@4': 11}, '(18, 18)': {'number@11': 13, 'result@12': [None], 'i@4': 12}, '(20, 500)': {'number@11': 13, 'result@12': True, 'i@4': 12}, 'keys': ['number@11', 'result@12', 'i@4']}\n",
      "<class 'dict'>\n",
      "          number@11 result@12     i@4\n",
      "(0, 1)       [None]    [None]  [None]\n",
      "(3, 6)           13    [None]  [None]\n",
      "(8, 8)           13    [None]       3\n",
      "(10, 10)         13    [None]       5\n",
      "(12, 12)         13    [None]       7\n",
      "(14, 14)         13    [None]       9\n",
      "(16, 16)         13    [None]      11\n",
      "(18, 18)         13    [None]      12\n",
      "(20, 500)        13      True      12\n",
      "  Time Range number@11 result@12     i@4\n",
      "0     (0, 1)    [None]    [None]  [None]\n",
      "1     (3, 6)        13    [None]  [None]\n",
      "2     (8, 8)        13    [None]       3\n",
      "3   (10, 10)        13    [None]       5\n",
      "4   (12, 12)        13    [None]       7\n",
      "5   (14, 14)        13    [None]       9\n",
      "6   (16, 16)        13    [None]      11\n",
      "7   (18, 18)        13    [None]      12\n",
      "8  (20, 500)        13      True      12\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Time Range  9 non-null      object\n",
      " 1   number@11   9 non-null      object\n",
      " 2   result@12   9 non-null      object\n",
      " 3   i@4         9 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 416.0+ bytes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [272], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [271], line 5\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m     data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(fileobj)\n\u001B[0;32m      4\u001B[0m csv_files \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv_files\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 5\u001B[0m dataframes \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataframes_from_csvs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_files\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m result_merge_condition \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresult_merge_condition\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      8\u001B[0m result_rows \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[0;32m      9\u001B[0m     dataframes[\u001B[38;5;241m0\u001B[39m], dataframes[\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m     10\u001B[0m     how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m     left_on\u001B[38;5;241m=\u001B[39mresult_merge_condition[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft_on\u001B[39m\u001B[38;5;124m\"\u001B[39m], right_on\u001B[38;5;241m=\u001B[39mresult_merge_condition[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright_on\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     12\u001B[0m     indicator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n",
      "Cell \u001B[1;32mIn [270], line 10\u001B[0m, in \u001B[0;36mcreate_dataframes_from_csvs\u001B[1;34m(csv_files)\u001B[0m\n\u001B[0;32m      8\u001B[0m server \u001B[38;5;241m=\u001B[39m PaladinServer\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, csv_archive)\n\u001B[0;32m      9\u001B[0m raw_result \u001B[38;5;241m=\u001B[39m server\u001B[38;5;241m.\u001B[39mquery(csv_file_info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m], csv_file_info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart_time\u001B[39m\u001B[38;5;124m\"\u001B[39m], csv_file_info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend_time\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m---> 10\u001B[0m presentable_df \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_query_result_to_presentable_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_result\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(presentable_df)\n\u001B[0;32m     12\u001B[0m dataframes\u001B[38;5;241m.\u001B[39mappend(presentable_df)\n",
      "Cell \u001B[1;32mIn [268], line 37\u001B[0m, in \u001B[0;36mconvert_query_result_to_presentable_table\u001B[1;34m(query_result)\u001B[0m\n\u001B[0;32m     35\u001B[0m result_df\u001B[38;5;241m.\u001B[39minfo(verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# result_df = result_df.replace([[None]], None)\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m result_df \u001B[38;5;241m=\u001B[39m result_df\u001B[38;5;241m.\u001B[39mreplace(to_replace\u001B[38;5;241m=\u001B[39m{[\u001B[38;5;28;01mNone\u001B[39;00m]: \u001B[38;5;28;01mNone\u001B[39;00m}, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(result_df)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result_df\n",
      "\u001B[1;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run:\n",
    "python C:\\Avital\\Github\\paladin_engine\\PaladinUI\\paladin_cli\\paladin_cli.py --run --output-file output.py --csv DiffTool\\is_prime_naive.csv --run-debug-server True --port 1234\n",
    "C:\\Avital\\Github\\paladin_engine\\PaladinEngine\\tests\\test_resources\\examples\\is_prime\\is_prime_naive.py\n",
    "\n",
    "#TODO:\n",
    "1) keys id: update the ctor\n",
    "2) time: 0 is converted to [1,5] (line_no==85)\n",
    "3) set: not supported\n",
    "\n",
    "#TODO: 28.11\n",
    "1) keys: id: update the ctor? ask Oren why it is needed id(v.key)\n",
    "2) store: write our function which doesn't change time\n",
    "3) represent: call represent asap (maybe in ctor) instead of in to_table\n",
    "4) paladin_server.py: debug_info/query/ - check that the Archive is OK\n",
    "\n",
    "#Questions:\n",
    "1) Will we always have exactly 2 tables to compare, or can there be more? A: let's start with 2, but can be more\n",
    "2) How exactly are we supposed to connect each two tables? For example in is_prime,\n",
    "should we just match the 13 rows of 'square' to the first 13 rows of 'naive'?\n",
    "3) User input - how? Using python's input(), or maybe read from file?\n",
    "Maybe should depend on the number of parameters in the query\n",
    "A: use files\n",
    "\n",
    "#TODO: 06.12\n",
    "1) not interactive, use files\n",
    "2) join two tables using merge/join pandas\n",
    "3) we get as input the query for creating the match between both tables (total_slices_1 == total_slices_2)\n",
    "4) represent: create object and check\n",
    "5) change set to list\n",
    "6) create more examples, more complex than is_prime\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}